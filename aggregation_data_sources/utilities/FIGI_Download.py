# ----------------------------------------------------------------------------------------------------
# FIGI_Download: This file starts from the universe of CUSIP9 identifiers generated by the script
# aggregation_data_sources/CGS.do and queries the OpenFIGI API in order to retrieve the FIGI identifiers 
# and other security-level information (e.g. security type) corresponding to each CUSIP9.
#
# Please note that this job takes a long time to run for the full univere of CUSIP9 identifiers 
# (roughly 1-2 weeks), and hence is provided as a separate utility on a stand-alone basis.
#
# Technical notes:
#
#   - Prior to running the procedure, please be sure to fill in the following parameter in the script:
#		<DATA_PATH>: Path to the folder containing the data, in which the procedure is executed
#
#	- Running this job requires an OpenFIGI API key. In order to run it, please register with
#       OpenFIGI and request a key; then enter this in place of <API_KEY> below.
# ----------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import logging
import requests
import time
import pickle
import os

# ----------------------------------------------------------------------------------------------------
# Define OpenFIGI API; uses code from https://github.com/jwergieluk/openfigis
# ----------------------------------------------------------------------------------------------------
class OpenFigi:
    id_types = {'ID_ISIN': 'ISIN',
                'ID_CUSIP': 'CUSIP',
                'ID_CINS': 'CINS - CUSIP International Numbering System'}

    def __init__(self, key=None):
        self.logger = logging.getLogger(__name__)
        self.url = 'https://api.openfigi.com/v2/mapping'
        self.api_key = key
        self.headers = {'Content-Type': 'text/json', 'X-OPENFIGI-APIKEY': key}
        self.request_items = []
        self.response_items = []
        self.max_tickers_per_request = 100

    def enqueue_request(self, id_type, id_value, exchange_code='', mic_code='', currency=''):
        query = {'idType': id_type, 'idValue': id_value}
        if id_type not in self.id_types.keys():
            self.logger.error('Bad id_type.')
            return
        if len(exchange_code) > 0:
            query['exchCode'] = exchange_code
        if len(mic_code) > 0:
            query['micCode'] = mic_code
        if len(currency) > 0:
            query['currency'] = currency

        self.request_items.append(query)

    def __get_batch(self, batch_request_items, remove_missing=False):
        response = requests.post(self.url, json=batch_request_items, headers=self.headers)
        try:
            response.raise_for_status()
        except requests.HTTPError:
            if response.status_code == 400:
                self.logger.error('The request body is not an array.')
            if response.status_code == 401:
                self.logger.error('The API_KEY is invalid.')
            if response.status_code == 404:
                self.logger.error('The requested path is invalid.')
            if response.status_code == 405:
                self.logger.error('The HTTP verb is not POST.')
            if response.status_code == 406:
                self.logger.error('The server does not support the requested Accept type.')
            if response.status_code == 413:
                self.logger.error('The request exceeds the max number of identifiers support in one request.')
            if response.status_code == 429:
                self.logger.error('Too Many Requests.')
            if response.status_code == 500:
                self.logger.error('Internal Server Error.')
            return None

        batch_response_items = response.json()
        if len(batch_response_items) == len(batch_request_items):
            for (i, item) in enumerate(batch_response_items):
                item.update(batch_request_items[i])
        else:
            self.logger.warning('Number of request and response items do not match. Dumping the results only.')
        if remove_missing:
            for item in batch_response_items:
                if 'error' not in item.keys():
                    self.response_items.append(item)
        else:
            self.response_items += batch_response_items

    def fetch_response(self, remove_missing=False):
        """ Partitions the requests into batches and attempts to get responses.

            See https://www.openfigi.com/api#rate-limiting for a detailed explanation.
        """
        if len(self.request_items) < 100:
            self.__get_batch(self.request_items, remove_missing)
        else:
            self.__get_batch(self.request_items[-100:], remove_missing)
            self.request_items = self.request_items[:-100]
            time.sleep(0.6)
            self.fetch_response(remove_missing)

        self.request_items.clear()
        return self.response_items

# Columns of interest for retrieval
cols = ['compositeFIGI', 'exchCode', 'figi', 'marketSector', 'name', 
        'securityDescription', 'securityType', 'securityType2', 
        'shareClassFIGI', 'ticker', 'uniqueID', 'uniqueIDFutOpt']

# ----------------------------------------------------------------------------------------------------
# Read in and partition the CGS data
# ----------------------------------------------------------------------------------------------------

# Utility function for partitioning
def chunks(l, n):
    for i in range(0, len(l), n):
        yield l[i:i + n]

# Change working directory
data_path = "<DATA_PATH>/cmns1/"
if os.path.isdir(data_path):
    os.chdir(data_path)
else:
    raise Exception("Path {} is not a valid directory".format(data_path))

# Read CGS data; this is produced by data_source_build/cgs.do
cusip_master = pd.read_stata("temp/cgs/all_cusips_universe_all_issuers.dta")
cusip_list = list(cusip_master['cusip'].values)
cusip_chunks = list(chunks(cusip_list, 500))
N_chunks = len(cusip_chunks)
print("There are {} chunks".format(N_chunks))

# Set up responses repository
response_dfs_dict_isin = {}

# Iterate over chunks and send requests to OpenFIGI
for i, chunk in enumerate(cusip_chunks):
    if i not in response_dfs_dict_cusip:
        attempts = 0
        while attempts < 10:
            try:

            	# Make request
                conn = OpenFigi("<API_KEY>")
                print("Processing chunk {} of {}".format(i, N_chunks))
                for cusip in chunk:
			        if cusip[0].isalpha():
			            conn.enqueue_request(id_type='ID_CINS', id_value=cusip)
			        else:
			            conn.enqueue_request(id_type='ID_CUSIP', id_value=cusip)
                response = conn.fetch_response()

                # Process request
                df = pd.DataFrame(response)
                if 'data' in df.columns:
                    for col in cols:
                        df[col] = [x[0][col] if type(x) != float else np.nan for x in df['data'] ]
                    del df['data']
                else:
                    print("\t\t\tWARNING: This chunk has no data")
                response_dfs_dict_cusip[i] = df
                time.sleep(1)
                break
         
         	# Error handling
            except:
                attempts += 1
                time.sleep(5)

# Prepare the appended dataframe
final_df_cusip = pd.concat(response_dfs_dict_cusip.values())
for col in ['securityDescription', 'ticker']:
    final_df_cusip[col].str.replace("\u20ac", "").replace('\u215b', "")

# Save the final output
final_df_cusip.rename(columns={'idValue': 'cusip'}).to_stata("raw/figi/figi_master_compact.dta")
